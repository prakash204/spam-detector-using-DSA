{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "N-TPYw5Y6eSJ",
        "sTJLy6_26eSK",
        "yXHu5_EU6eSL",
        "bf-x4ctq6eSA",
        "1ImFApeC6eSE"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5eag6IU6eRd",
        "outputId": "5f29eacf-50db-4501-8997-5fa3d453363c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SEZkw8I6eRo"
      },
      "source": [
        "## Reading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEKMc_0r6eRu"
      },
      "source": [
        "black_list_df = pd.read_csv(\"dataset/black_list.csv\",names=['blacklist'])\n",
        "words_df = pd.read_csv(\"dataset/spam_words.csv\",names=['spam_words'])\n",
        "dataset = pd.read_csv('dataset/spam_ham_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbHBfMA-6eRw"
      },
      "source": [
        "## Preprocessing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84dmYLXh6eRx"
      },
      "source": [
        "### A. Black list dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIowDMIV6eRx"
      },
      "source": [
        "black_list = black_list_df['blacklist'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKRumHkR6eRy"
      },
      "source": [
        "def process_email(email_id):\n",
        "    email_id = email_id.replace('@','')\n",
        "    email_id = email_id.replace('.','')\n",
        "    email_id = email_id.replace('+','')\n",
        "    email_id = email_id.replace('/','')\n",
        "    email_id = email_id.replace('-','')\n",
        "    email_id = email_id.replace('\"','')\n",
        "    return email_id\n",
        "\n",
        "n = len(black_list)\n",
        "for i in range(n):\n",
        "    black_list[i] = process_email(black_list[i])\n",
        "\n",
        "white_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvQtrJ5_6eR3"
      },
      "source": [
        "### B. Spam words dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "QV26EYeD6eR5",
        "outputId": "d286674a-13d0-4855-dc34-8e181fdf9dcb"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "def clean_str(string, reg = RegexpTokenizer(r'[a-z]+')):\n",
        "    string = string.lower()\n",
        "    tokens = reg.tokenize(string)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "words_df['spam_words'] = words_df['spam_words'].apply(lambda string: clean_str(string))\n",
        "\n",
        "words_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spam_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>text</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hidden assets</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>risk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>free</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>more</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571</th>\n",
              "      <td>your income</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572</th>\n",
              "      <td>your status</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>zero chance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>574</th>\n",
              "      <td>zero percent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>575</th>\n",
              "      <td>zero risk</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>576 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        spam_words\n",
              "0             text\n",
              "1    hidden assets\n",
              "2             risk\n",
              "3             free\n",
              "4             more\n",
              "..             ...\n",
              "571    your income\n",
              "572    your status\n",
              "573    zero chance\n",
              "574   zero percent\n",
              "575      zero risk\n",
              "\n",
              "[576 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv6u7WCb6eR8"
      },
      "source": [
        "nwords = words_df['spam_words'].tolist()\n",
        "spam_words = [i for i in nwords if len(i)>=5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KwJG0Qc6eR9"
      },
      "source": [
        "### C. Spam-Ham dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3rHCLZ-6eR9"
      },
      "source": [
        "dataset['text'] = dataset['text'].apply(lambda string: clean_str(string))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "EEGNM0Vq6eR-",
        "outputId": "159cf01c-a6b6-4fc5-aa46-e3458b1989b0"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "def remove_stopwords(text):  \n",
        "    stop_words = stopwords.words('english')\n",
        "    stop_words.append('subject')\n",
        "    words = text.lower().split()\n",
        "    words = [word for word in words if word not in stop_words and len(word) >= 5]    \n",
        "    text_new = ' '.join(words)\n",
        "    \n",
        "    return text_new\n",
        "\n",
        "dataset['text'] = dataset.text.apply(remove_stopwords)\n",
        "dataset.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>enron methanol meter follow monday preliminary...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>january attached hplnol hplnol</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>retreat around wonderful leaders retreat extre...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>photoshop windows office cheap trending abasem...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>indian springs revenue understanding sends che...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... label_num\n",
              "0         605  ...         0\n",
              "1        2349  ...         0\n",
              "2        3624  ...         0\n",
              "3        4685  ...         1\n",
              "4        2030  ...         0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lzwFOUc6eR_"
      },
      "source": [
        "spam_df = pd.read_csv('dataset/spam.csv')\n",
        "ham_df = pd.read_csv('dataset/ham.csv')\n",
        "\n",
        "text = dataset['text'].tolist()\n",
        "label = dataset['label_num'].tolist()\n",
        "words = spam_df['word'].tolist()\n",
        "p_ham = ham_df['score'].tolist()\n",
        "p_spam = spam_df['score'].tolist()\n",
        "\n",
        "prob_spam = {}\n",
        "prob_ham = {}\n",
        "\n",
        "n1 = len(words)\n",
        "for i in range(0,n1):\n",
        "    words[i] = str(words[i])\n",
        "    prob_spam[words[i]] = p_spam[i]\n",
        "    prob_ham[words[i]] = p_ham[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-TPYw5Y6eSJ"
      },
      "source": [
        "## Rabin-Karp algorithm "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTJLy6_26eSK"
      },
      "source": [
        "### A. Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcxYkbIC6eSK"
      },
      "source": [
        "def rabin_karp_match(string, string_list, modulo_number):\n",
        "    total = 256\n",
        "    n = len(string_list) # length of the string list\n",
        "    string = str(string) \n",
        "    l = len(string) # string length\n",
        "    k=0\n",
        "    for k in range(n):\n",
        "        text = string_list[k] # element of the string_list\n",
        "        N=len(text)\n",
        "        i = 0\n",
        "        j = 0\n",
        "        s_num = 0   \n",
        "        t_num = 0    \n",
        "        hash_f = 1\n",
        "        flag=0\n",
        "        if(N==l):\n",
        "            for i in range(l-1):\n",
        "                hash_f = (hash_f * total)% modulo_number\n",
        "            for i in range(l):\n",
        "                s_num = (total * s_num + ord(string[i]))% modulo_number\n",
        "                t_num = (total * t_num + ord(text[i]))% modulo_number\n",
        "            for i in range(N-l + 1):\n",
        "                if s_num == t_num:\n",
        "                    for j in range(l):\n",
        "                        if text[i + j] != string[j]:\n",
        "                            break  \n",
        "                    j+= 1          \n",
        "                    if j == l:\n",
        "                        return 1\n",
        "                if i < N-l:\n",
        "                    t_num = (d*(t_num-ord(text[i])*hash_f) + ord(text[i + l]))% modulo_number\n",
        "                    if t_num < 0:\n",
        "                        t_num = t_num + modulo_number\n",
        "    return 0\n",
        "\n",
        "def rabin_karp_pattern(pattern, text, modulo_number):\n",
        "    total = 256\n",
        "    pattern = str(pattern)\n",
        "    M = len(pattern) # pattern length\n",
        "    N = len(text) # text length\n",
        "    i = 0\n",
        "    j = 0\n",
        "    p_num = 0   \n",
        "    t_num = 0    \n",
        "    hash_f = 1\n",
        "    flag=0\n",
        "\n",
        "    for i in range(M-1):\n",
        "        hash_f = (hash_f * total)% modulo_number\n",
        "    for i in range(M):\n",
        "        p_num = (total * p_num + ord(pattern[i]))% modulo_number\n",
        "        t_num = (total * t_num + ord(text[i]))% modulo_number\n",
        "    for i in range(N-M + 1):\n",
        "        if p_num == t_num:\n",
        "            for j in range(M):\n",
        "                if text[i + j] != pattern[j]:\n",
        "                    break\n",
        "  \n",
        "            j+= 1\n",
        "            if j == M:\n",
        "                return 1\n",
        "        if i < N-M:\n",
        "            t_num = (total*(t_num-ord(text[i])*hash_f) + ord(text[i + M]))% modulo_number\n",
        "            if t_num < 0:\n",
        "                t_num = t_num + modulo_number\n",
        "\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXHu5_EU6eSL"
      },
      "source": [
        "### B. Black list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAt23wsk6eSL"
      },
      "source": [
        "def check_in_email(sender,email_list):\n",
        "  return rabin_karp_match(sender,email_list,101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lQYFwp16eR_"
      },
      "source": [
        "## KMP algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf-x4ctq6eSA"
      },
      "source": [
        "### A. Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DBNuOdL6eSB"
      },
      "source": [
        "def computeLPSArray(pat, M, lps):\n",
        "    len = 0\n",
        "    lps[0] = 0\n",
        "    i = 1\n",
        "    while i < M:\n",
        "        if pat[i] == pat[len]:\n",
        "            len += 1\n",
        "            lps[i] = len\n",
        "            i += 1\n",
        "        else:\n",
        "            if len != 0:\n",
        "                len = lps[len - 1]\n",
        "            else:\n",
        "                lps[i] = 0\n",
        "                i += 1\n",
        "\n",
        "def KMP(pat, txt):\n",
        "    M = len(pat)\n",
        "    N = len(txt)\n",
        "    count = 0\n",
        "    lps = [0] * M\n",
        "    j = 0 \n",
        "    computeLPSArray(pat, M, lps)\n",
        "    i = 0 \n",
        "    while i < N:\n",
        "        if pat[j] == txt[i]:\n",
        "            i += 1\n",
        "            j += 1\n",
        "        if j == M:\n",
        "            count += 1\n",
        "            j = lps[j - 1]\n",
        "        elif i < N and pat[j] != txt[i]:\n",
        "            if j != 0:\n",
        "                j = lps[j - 1]\n",
        "            else:\n",
        "                i += 1\n",
        "    return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ImFApeC6eSE"
      },
      "source": [
        "### C. Word based method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkgO6gn-6eSF"
      },
      "source": [
        "def check_spam_text_message(words,text):\n",
        "  text = clean_str(text)\n",
        "  for i in range(len(words)):\n",
        "    if(KMP(words[i],text)!=0):\n",
        "      return True\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs_YWGNN6eST"
      },
      "source": [
        "## Compressed Tries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ixiem0X6eST"
      },
      "source": [
        "### A. Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_guDwp36eSU"
      },
      "source": [
        "class TrieNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.is_end = True\n",
        "\n",
        "class Trie:\n",
        "    def __init__(self):\n",
        "        #root node in trie\n",
        "        self.root = self.getNewNode()\n",
        "\n",
        "    def getNewNode(self):\n",
        "        return TrieNode()\n",
        "\n",
        "    def insert(self,word):\n",
        "        present_node = self.root\n",
        "        length = len(word)    \n",
        "        i = 0\n",
        "        tt = False\n",
        "        while i < (length):\n",
        "            # if current character is not present\n",
        "            children = list(present_node.children.keys())\n",
        "            index = -1\n",
        "            if tt:\n",
        "                print(children)\n",
        "            for j in range(len(children)):\n",
        "                stri = children[j]\n",
        "                #print(stri,end='\\n')\n",
        "                if(stri[0] == word[i]):\n",
        "                    index = j\n",
        "                    break\n",
        "      \n",
        "            if index == -1:\n",
        "                present_node.children[word[i:]] = self.getNewNode()\n",
        "                break\n",
        "\n",
        "            else :\n",
        "                #gather path starting from word[i]\n",
        "                pointe = children[index]#already existing path\n",
        "                #word[i:] needed to be inserted\n",
        "                j = 0\n",
        "                k = i\n",
        "                flag = False\n",
        "                while j < (len(pointe)):\n",
        "                    if(word[k] != pointe[j]):\n",
        "                        paths = list(present_node.children.keys())\n",
        "                        same_node = pointe[:j]\n",
        "                        present_node.children[same_node] = self.getNewNode()\n",
        "\n",
        "                        temp_node = present_node.children[pointe]\n",
        "                        del present_node.children[pointe]\n",
        "            \n",
        "                        present_node = present_node.children[same_node]\n",
        "                        present_node.children[pointe[j:]] = temp_node\n",
        "                        present_node.children[word[k:]] = self.getNewNode()\n",
        "\n",
        "                        #print(paths)\n",
        "                        #print(\"Inserted2 {}\".format(word[k:]))\n",
        "                        flag = True\n",
        "                        break\n",
        "                    j += 1\n",
        "                    k += 1\n",
        "                if(flag) :\n",
        "                    break\n",
        "                else:\n",
        "                    present_node = present_node.children[pointe]\n",
        "                    i = k-1\n",
        "            i += 1\n",
        "\n",
        "    def search(self, pattern):\n",
        "        pattern = str(pattern)\n",
        "        present_node = self.root\n",
        "        length = len(pattern)\n",
        "        i = 0\n",
        "        boo = False\n",
        "        while i < length:\n",
        "            paths = list(present_node.children.keys())\n",
        "            a = present_node.children.get(pattern[i])\n",
        "            l= len(pattern[i:])\n",
        "            b = None\n",
        "            nex = pattern[i:]\n",
        "            k = len(pattern[i:])\n",
        "            while(k>=0):\n",
        "                b = present_node.children.get(pattern[i:i+k])\n",
        "                nex = pattern[i:i+k]\n",
        "                if not b == None:\n",
        "                    break\n",
        "                k -= 1\n",
        "            if a==b:\n",
        "                b = None\n",
        "            if (a == None and b == None ):\n",
        "                return False\n",
        "            elif b == None :\n",
        "                if pattern[i] == '$':\n",
        "                    return True\n",
        "                present_node = present_node.children[pattern[i]]\n",
        "                i += 1\n",
        "            else:\n",
        "                if nex == pattern[i:]:\n",
        "                    return True\n",
        "                else:\n",
        "                    present_node = present_node.children[nex]\n",
        "                    i += len(nex)\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0A9_STsMd8B"
      },
      "source": [
        "def classify_tries(message,words_trie, prob_spam, prob_ham):\n",
        "    ps = 0.2864394488759971 \n",
        "    ph = 0.7135605511240029\n",
        "\n",
        "    message = message.split(' ')\n",
        "    \n",
        "    for word in message:\n",
        "      if words_trie.search(word+'$'):\n",
        "        ps *= prob_spam[word]\n",
        "        ph *= prob_ham[word]\n",
        "    print(ps, ph)\n",
        "    if ps == 0 or ph == 0 :\n",
        "      rp_spam = 0\n",
        "    else:\n",
        "      rp_spam = ps/(ps+ph)\n",
        "    print(rp_spam)\n",
        "    if(rp_spam>=0.95):\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBaZVJldMjxp"
      },
      "source": [
        "words_trie = Trie()\n",
        "for word in words:\n",
        "  words_trie.insert(word+'$')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK0eFL_aJVB-"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEi1cVkdJUKT"
      },
      "source": [
        "email_id = input(\"Enter email address : \")\n",
        "email_content = input(\"Enter email content : \")\n",
        "\n",
        "if check_in_email(email_id,black_list):\n",
        "  print(\"Spam, email id in blacklist\")\n",
        "elif check_in_email(email_id,white_list):\n",
        "  print(\"Ham, email id in White list\")\n",
        "elif check_spam_text_message(spam_words,email_content):\n",
        "  print(\"Spam, email content seems to be spam by word-based\")\n",
        "  black_list.append(process_email(email_id))\n",
        "elif classify_tries(email_content, words_trie, prob_spam, prob_ham):\n",
        "  print(\"Spam, email content seems to be spam by Heuristic method\")\n",
        "  black_list.append(process_email(email_id))\n",
        "elif not check_in_email(email_id,white_list):\n",
        "  print(\"Ham\")\n",
        "  white_list.append(process_email(email_id))\n",
        "else:\n",
        "  print(\"Ham\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}